import openai
import pandas as pd
import os
from typing import List, Dict, Any
import re
import json
from datetime import datetime
import time


class PunsGenerator:
    def __init__(self, openai_api_key: str, maritaca_api_key: str, use_sabia_generate: bool = False):
        """
        Inicializa o gerador com controle do modelo de geraÃ§Ã£o (GPT ou SabiÃ¡).
        AnÃ¡lises sempre serÃ£o feitas por ambos os modelos.
        """
        self.use_sabia_generate = use_sabia_generate
        self.df = self._load_or_create_dataframe()

        # Clientes
        openai.api_key = openai_api_key
        self.client_gpt = openai

        self.client_sabia = openai.OpenAI(
            api_key=maritaca_api_key,
            base_url="https://chat.maritaca.ai/api"
        )

    def _load_or_create_dataframe(self) -> pd.DataFrame:
        if os.path.exists(os.path.abspath('files\puns_sintetico\puns_history.csv')):
            return pd.read_csv(os.path.abspath('files\puns_sintetico\puns_history.csv'))
        # agora com as colunas de estilo incluÃ­das
        return pd.DataFrame(columns=[
            'timestamp', 'puns',
            'pun_bin_sabia', 'pun_style_sabia', 'analyses_sabia',
            'pun_bin_gpt',   'pun_style_gpt',   'analyses_gpt'
        ])

    def generate_puns(self, promtp_generation: str) -> List[str]:
        """
        Gera trocadilhos com GPT ou SabiÃ¡.
        """

        client = self.client_sabia if self.use_sabia_generate else self.client_gpt
        model = "sabia-3.1" if self.use_sabia_generate else "gpt-4o"

        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": promtp_generation}],
            temperature=0.7,
            max_tokens=7000
        )

        puns = response.choices[0].message.content.strip().split('\n')
        return [pun.strip() for pun in puns if pun.strip()]

    def _clean_json_response(self, response_text: str) -> str:
        """
        Extrai JSON bruto da resposta.
        """
        json_pattern = r'```(?:json)?\s*([\s\S]*?)```'
        matches = re.findall(json_pattern, response_text)
        return matches[0].strip() if matches else response_text.strip()

    def analyze_puns(self, puns: List[str], prompt_recognition_2: str, model_name: str) -> List[Dict[str, str]]:
        """
        Analisa trocadilhos com o modelo especificado: 'sabia' ou 'gpt'
        """
        prompt_recognition = """VocÃª Ã© um analista especializado em reconhecimento e avaliaÃ§Ã£o de humor linguÃ­stico, com foco especÃ­fico em trocadilhos.

- DefiniÃ§Ã£o: Trocadilhos sÃ£o jogos de palavras que exploram ambiguidade semÃ¢ntica com a mesma grafia e mÃºltiplos significados (trocadilhos homogrÃ¡ficos) ou a semelhanÃ§a sonora entre palavras de grafia distinta (trocadilhos heterogrÃ¡ficos) com objetivo Ã© causar surpresa e humor inteligente.

Exemplos de Trocadilhos HomogrÃ¡ficos:
â€¢ Deve ser difÃ­cil ser professor de nataÃ§Ã£o. VocÃª ensina, ensina, e o aluno nada.
â€¢ Como Ã© que os sapateiros entram nas piscinas? Fazem um salto.
â€¢ Por que o carro do dentista foi apreendido? Porque ele estava sem placa.
â€¢ Porque Ã© que os indecisos nÃ£o atravessam o rio? Porque nÃ£o hÃ¡ margem para dÃºvidas.
â€¢ Se honestidade responsabilidade e humildade pudessem ser negociados, onde se daria essa negociaÃ§Ã£o? Na bolsa de valores.

Exemplos de Trocadilhos HeterogrÃ¡ficos:
â€¢ Qual o animal que trabalha bem como carpinteiro? Rinocerrote.
â€¢ Qual Ã¡rvore ficou de saco cheio e mandou vocÃª embora? Bom... sai.
â€¢ Qual a cidade que estÃ¡ inovando sua forma de comunicaÃ§Ã£o? Emoji Mirim.
â€¢ Como se chamava a apresentadora do primeiro programa feito para a internet? Web Camargo.
â€¢ Como se chama o cara que ficou rico vendendo pipoca? MilhonÃ¡rio.

Tarefa: Determinar se o conteÃºdo atende aos critÃ©rios para ser classificado como um trocadilho, considerando a defiÃ§Ã£o imposta e nos exemplos dados. Avalie a estrutura e o uso de humor de cada frase fornecida. ApÃ³s isso, guarde a reposta na sua memÃ³ria de "SIM" ou "NÃƒO".

â€¢ Se a resposta de ser um trocadilho for "NÃƒO", sem realizar as anÃ¡lises posteriores, retorne imediatamente o JSON com:
{
  "pun_bin": "NÃƒO",
  "pun": "",
  "analysis": ""
}

â€¢ Se a resposta de ser um trocadilho for "SIM", explique o trocadilho e de onde que vem o carÃ¡ter humorÃ­stico dele. Formato obrigatÃ³rio de saÃ­da: Retorne todas as respostas em JSON, como descrito abaixo:
{
  "ratings": [
    {
      "pun_bin": "SIM",
      "pun_style": "TROCADILHO HOMOGRAFICO" ou "TROCADILHO HETEROGRAFICO",
      "pun": "texto do trocadilho",
      "analysis": "breve anÃ¡lise crÃ­tica"
    },
    {
      "pun_bin": "NÃƒO",
      "pun_style": ""
      "pun": "",
      "analysis": ""
    }
    // repita conforme necessÃ¡rio
  ]
}"""
        prompt_recognition += f"\nTextos para anÃ¡lise:\n{json.dumps(puns, indent=2, ensure_ascii=False)}"

        if model_name == "sabia":
            client = self.client_sabia
            model = "sabia-3.1"
        else:
            client = self.client_gpt
            model = "gpt-4o"

        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt_recognition}],
            temperature=0.7,
            max_tokens=4000
        )

        content = response.choices[0].message.content
        cleaned = self._clean_json_response(content)

        data = json.loads(cleaned)
        return data.get("ratings", [])


    def save_to_dataframe(self, ratings_sabia: List[Dict[str, Any]], ratings_gpt: List[Dict[str, Any]]) -> None:
        """
        Salva resultados de ambas as anÃ¡lises no DataFrame.
        """
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        rows = []

        min_len = min(len(ratings_sabia), len(ratings_gpt))

        for i in range(min_len):
            rows.append({
                'timestamp': timestamp,
                'puns': ratings_sabia[i].get('pun', '') or ratings_gpt[i].get('pun', ''),
                'pun_bin_sabia': ratings_sabia[i].get('pun_bin', ''),
                'pun_style_sabia': ratings_sabia[i].get('pun_style', ''),
                'analyses_sabia': ratings_sabia[i].get('analysis', ''),
                'pun_bin_gpt': ratings_gpt[i].get('pun_bin', ''),
                'pun_style_gpt': ratings_gpt[i].get('pun_style', ''),
                'analyses_gpt': ratings_gpt[i].get('analysis', '')
            })

        self.df = pd.concat([self.df, pd.DataFrame(rows)], ignore_index=True)
        self.df.to_csv(os.path.abspath('files\puns_sintetico\puns_history.csv'), index=False)

    def run_batch_process(self, prompt_generation, prompt_recognition: str, num_batches: int = 1):
        print(f"ðŸŽ­ Rodando {num_batches} batches...\n")

        for i in range(num_batches):
            print(f"ðŸ“¦ Batch {i+1}/{num_batches}")
            print("-" * 50)

            try:
                puns = self.generate_puns(prompt_generation)
                print(puns)
                print(f"ðŸ”¤ Gerados {len(puns)} trocadilhos.")

                ratings_sabia = self.analyze_puns(puns, prompt_recognition, model_name="sabia")
                ratings_gpt = self.analyze_puns(puns, prompt_recognition, model_name="gpt")

                self.save_to_dataframe(ratings_sabia, ratings_gpt)
                print("âœ… Resultados salvos!\n")

                if i < num_batches - 1:
                    time.sleep(2)

            except Exception as e:
                print(f"âŒ Erro no batch {i+1}: {str(e)}")
                import traceback
                traceback.print_exc()

        print("ðŸ Finalizado!")

    def get_history(self) -> pd.DataFrame:
        return self.df

